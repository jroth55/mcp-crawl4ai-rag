# Deployment Guide

This guide covers various deployment options for the Crawl4AI RAG MCP Server, from simple local deployments to production-ready cloud configurations.

## Deployment Options Overview

| Method | Best For | Complexity | Cost |
|--------|----------|------------|------|
| Docker Compose | Local/small deployments | Low | Free |
| Coolify | Self-hosted PaaS | Medium | VPS costs |
| Railway/Render | Quick cloud deployment | Low | Usage-based |
| Kubernetes | Large-scale production | High | Infrastructure costs |
| AWS/GCP/Azure | Enterprise deployment | High | Pay-as-you-go |

## Docker Deployment

### Basic Docker Run

```bash
# Build the image
docker build -t mcp/crawl4ai-rag .

# Run with environment file
docker run -d \
  --name crawl4ai-rag \
  --env-file .env \
  -p 8051:8051 \
  --restart unless-stopped \
  mcp/crawl4ai-rag
```

### Docker Compose

Create `docker-compose.yml`:

```yaml
version: '3.8'

services:
  crawl4ai-rag:
    build: 
      context: .
      args:
        PORT: 8051
    container_name: crawl4ai-rag
    ports:
      - "8051:8051"
    env_file:
      - .env
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8051/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

Run with:
```bash
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

### Production Docker Considerations

```yaml
version: '3.8'

services:
  crawl4ai-rag:
    build: .
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    environment:
      - HOST=0.0.0.0
      - PORT=8051
      - TRANSPORT=sse
      - MCP_SERVER_API_KEY=${MCP_SERVER_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      # Performance tuning
      - CRAWLER_MEMORY_THRESHOLD=80.0
      - CRAWLER_TIMEOUT=45000
      - MAX_DOCUMENT_LENGTH=30000
    volumes:
      # Optional: Persist logs
      - ./logs:/app/logs
    networks:
      - mcp-network

networks:
  mcp-network:
    driver: bridge
```

## Coolify Deployment

[Coolify](https://coolify.io) is an open-source, self-hostable Heroku/Netlify alternative.

### Prerequisites

1. A VPS with Coolify installed
2. A domain name (optional but recommended)
3. SSL certificates (auto-generated by Coolify)

### Step-by-Step Coolify Deployment

1. **Create New Project** in Coolify dashboard

2. **Add New Resource** â†’ Choose "Docker"

3. **Configure Git Repository**:
   - Repository: `https://github.com/coleam00/mcp-crawl4ai-rag.git`
   - Branch: `main`
   - Build Pack: Dockerfile

4. **Set Environment Variables**:
   ```
   HOST=0.0.0.0
   PORT=8051
   TRANSPORT=sse
   MCP_SERVER_API_KEY=your_secure_api_key
   OPENAI_API_KEY=your_openai_key
   SUPABASE_URL=your_supabase_url
   SUPABASE_SERVICE_KEY=your_service_key
   ```

5. **Configure Domains**:
   - Add your domain: `crawl4ai.yourdomain.com`
   - Enable HTTPS (automatic with Let's Encrypt)

6. **Advanced Settings**:
   - Health Check URL: `/health`
   - Port: 8051
   - Resource Limits: 2GB RAM minimum

7. **Deploy** and monitor logs

### Connecting Claude Code to Coolify

```bash
claude mcp add --transport sse crawl4ai-rag \
  -e X-API-Key=your_mcp_server_api_key \
  https://crawl4ai.yourdomain.com/sse
```

## Cloud Platform Deployments

### Railway

1. **Install Railway CLI**:
   ```bash
   npm install -g @railway/cli
   ```

2. **Deploy**:
   ```bash
   railway login
   railway init
   railway up
   ```

3. **Add environment variables** in Railway dashboard

4. **Get deployment URL** from Railway

### Render

1. **Create `render.yaml`**:
   ```yaml
   services:
     - type: web
       name: crawl4ai-rag
       env: docker
       dockerfilePath: ./Dockerfile
       envVars:
         - key: HOST
           value: 0.0.0.0
         - key: PORT
           value: 8051
         - key: TRANSPORT
           value: sse
         - key: MCP_SERVER_API_KEY
           generateValue: true
         - key: OPENAI_API_KEY
           sync: false
         - key: SUPABASE_URL
           sync: false
         - key: SUPABASE_SERVICE_KEY
           sync: false
   ```

2. **Connect GitHub repository** in Render dashboard

3. **Deploy** and configure environment variables

### Google Cloud Run

```bash
# Build and push to Container Registry
gcloud builds submit --tag gcr.io/PROJECT-ID/crawl4ai-rag

# Deploy to Cloud Run
gcloud run deploy crawl4ai-rag \
  --image gcr.io/PROJECT-ID/crawl4ai-rag \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --set-env-vars="TRANSPORT=sse" \
  --set-env-vars="MCP_SERVER_API_KEY=your_key" \
  --memory 2Gi \
  --cpu 2 \
  --timeout 300 \
  --concurrency 100
```

## Kubernetes Deployment

### Basic Kubernetes Manifests

Create `k8s/deployment.yaml`:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crawl4ai-rag
  labels:
    app: crawl4ai-rag
spec:
  replicas: 3
  selector:
    matchLabels:
      app: crawl4ai-rag
  template:
    metadata:
      labels:
        app: crawl4ai-rag
    spec:
      containers:
      - name: crawl4ai-rag
        image: mcp/crawl4ai-rag:latest
        ports:
        - containerPort: 8051
        env:
        - name: HOST
          value: "0.0.0.0"
        - name: PORT
          value: "8051"
        - name: TRANSPORT
          value: "sse"
        envFrom:
        - secretRef:
            name: crawl4ai-secrets
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8051
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8051
          initialDelaySeconds: 5
          periodSeconds: 5
```

Create `k8s/service.yaml`:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: crawl4ai-rag
spec:
  selector:
    app: crawl4ai-rag
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8051
  type: LoadBalancer
```

Create `k8s/secret.yaml`:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: crawl4ai-secrets
type: Opaque
stringData:
  MCP_SERVER_API_KEY: "your_api_key"
  OPENAI_API_KEY: "your_openai_key"
  SUPABASE_URL: "your_supabase_url"
  SUPABASE_SERVICE_KEY: "your_service_key"
```

Deploy:

```bash
# Create namespace
kubectl create namespace mcp

# Apply configurations
kubectl apply -f k8s/ -n mcp

# Check status
kubectl get pods -n mcp
kubectl get service -n mcp
```

### Helm Chart

Create `helm/crawl4ai-rag/values.yaml`:

```yaml
replicaCount: 3

image:
  repository: mcp/crawl4ai-rag
  tag: latest
  pullPolicy: IfNotPresent

service:
  type: LoadBalancer
  port: 80

ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: crawl4ai.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: crawl4ai-tls
      hosts:
        - crawl4ai.example.com

resources:
  limits:
    cpu: 2000m
    memory: 4Gi
  requests:
    cpu: 1000m
    memory: 2Gi

env:
  HOST: "0.0.0.0"
  PORT: "8051"
  TRANSPORT: "sse"

secrets:
  MCP_SERVER_API_KEY: ""
  OPENAI_API_KEY: ""
  SUPABASE_URL: ""
  SUPABASE_SERVICE_KEY: ""

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80
```

## Reverse Proxy Configuration

### Nginx

```nginx
server {
    listen 80;
    server_name crawl4ai.example.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name crawl4ai.example.com;

    ssl_certificate /etc/letsencrypt/live/crawl4ai.example.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/crawl4ai.example.com/privkey.pem;

    # SSE specific settings
    location /sse {
        proxy_pass http://localhost:8051/sse;
        proxy_http_version 1.1;
        proxy_set_header Connection '';
        proxy_set_header Cache-Control 'no-cache';
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # SSE settings
        proxy_buffering off;
        proxy_cache off;
        proxy_read_timeout 86400s;
        keepalive_timeout 90s;
        
        # CORS headers if needed
        add_header Access-Control-Allow-Origin "*";
        add_header Access-Control-Allow-Headers "X-API-Key, Content-Type";
    }

    location / {
        proxy_pass http://localhost:8051;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

### Apache

```apache
<VirtualHost *:443>
    ServerName crawl4ai.example.com
    
    SSLEngine on
    SSLCertificateFile /etc/letsencrypt/live/crawl4ai.example.com/cert.pem
    SSLCertificateKeyFile /etc/letsencrypt/live/crawl4ai.example.com/privkey.pem
    
    # SSE specific
    <Location /sse>
        ProxyPass http://localhost:8051/sse
        ProxyPassReverse http://localhost:8051/sse
        
        # Disable buffering for SSE
        SetEnv proxy-initial-not-pooled 1
        SetEnv proxy-nokeepalive 0
        SetEnv force-proxy-request-1.0 0
        SetEnv proxy-sendcl 0
        
        # Headers
        Header set Cache-Control "no-cache"
        Header set Access-Control-Allow-Origin "*"
        Header set Access-Control-Allow-Headers "X-API-Key, Content-Type"
    </Location>
    
    ProxyPass / http://localhost:8051/
    ProxyPassReverse / http://localhost:8051/
</VirtualHost>
```

## Monitoring and Logging

### Basic Health Check

```bash
# Health endpoint
curl http://localhost:8051/health

# With authentication
curl -H "X-API-Key: your_api_key" https://crawl4ai.example.com/health
```

### Prometheus Metrics

Add to your deployment:

```python
# In crawl4ai_mcp.py (future enhancement)
from prometheus_client import Counter, Histogram, generate_latest

crawl_counter = Counter('crawl_requests_total', 'Total crawl requests')
search_histogram = Histogram('search_duration_seconds', 'Search request duration')

@mcp.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")
```

### Logging Configuration

```python
# Enhanced logging setup
import logging
from logging.handlers import RotatingFileHandler

# Configure logging
logger = logging.getLogger(__name__)
handler = RotatingFileHandler(
    'logs/crawl4ai.log',
    maxBytes=10485760,  # 10MB
    backupCount=5
)
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.INFO)
```

## Security Best Practices

### Environment Variables

1. **Never commit secrets** to version control
2. **Use secret management**:
   - Kubernetes Secrets
   - AWS Secrets Manager
   - HashiCorp Vault
   - Docker Secrets

### Network Security

```yaml
# Kubernetes NetworkPolicy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: crawl4ai-netpol
spec:
  podSelector:
    matchLabels:
      app: crawl4ai-rag
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: nginx-ingress
    ports:
    - protocol: TCP
      port: 8051
  egress:
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443  # HTTPS for external APIs
    - protocol: TCP
      port: 5432 # PostgreSQL for Supabase
```

### API Key Rotation

```bash
# Script for key rotation
#!/bin/bash

# Generate new API key
NEW_KEY=$(openssl rand -hex 32)

# Update in secret manager
kubectl create secret generic crawl4ai-secrets \
  --from-literal=MCP_SERVER_API_KEY=$NEW_KEY \
  --dry-run=client -o yaml | kubectl apply -f -

# Restart pods
kubectl rollout restart deployment/crawl4ai-rag
```

## Scaling Considerations

### Horizontal Scaling

```yaml
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: crawl4ai-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: crawl4ai-rag
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### Database Connection Pooling

```python
# In Supabase client setup
SUPABASE_POOL_SIZE = int(os.getenv("SUPABASE_POOL_SIZE", "10"))
```

## Backup and Disaster Recovery

### Database Backups

Supabase provides automatic backups, but for additional safety:

```bash
# Backup crawled content
pg_dump $DATABASE_URL -t crawled_pages > backup_$(date +%Y%m%d).sql

# Restore
psql $DATABASE_URL < backup_20240115.sql
```

### Application State

The MCP server is stateless, making recovery simple:

1. Restore database from backup
2. Redeploy application
3. No session state to recover

## Cost Optimization

### Resource Recommendations

| Deployment Size | CPU | Memory | Concurrent Crawls |
|----------------|-----|--------|-------------------|
| Small | 1 core | 2GB | 5 |
| Medium | 2 cores | 4GB | 10 |
| Large | 4 cores | 8GB | 20 |

### Cost-Saving Tips

1. **Use spot/preemptible instances** for non-critical deployments
2. **Implement request caching** to reduce API calls
3. **Set appropriate crawl limits** to prevent runaway costs
4. **Monitor and alert** on resource usage

## Troubleshooting Deployment Issues

### Common Issues

1. **"Cannot connect to Supabase"**
   - Check network connectivity
   - Verify credentials
   - Ensure firewall rules allow outbound HTTPS

2. **"Out of memory" errors**
   - Increase container memory limits
   - Reduce `max_concurrent` crawls
   - Enable memory monitoring

3. **"SSE connection drops"**
   - Check proxy timeout settings
   - Ensure keepalive is configured
   - Verify SSL certificate validity

See [Troubleshooting Guide](troubleshooting.md) for more details.

## Next Steps

- Set up monitoring with [Grafana](https://grafana.com/)
- Implement CI/CD pipeline
- Configure automated backups
- Plan for disaster recovery